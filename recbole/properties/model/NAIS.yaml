# WARNING:
# if your dataset has too many items and you dont't have enough GPU memory to evaluate it
# by using full sort, you should set 'split_to' > 0 and try again. Otherwise, you should
# set 'split_to'=0 which has the fastest speed.

algorithm: prod
# algorithm=concat
embedding_size: 64
weight_size: 64
# when evaluate the model by full sort, one user will be divide into the number of slices you set
split_to: 0
reg_weights: [1e-7, 1e-7, 1e-5]
alpha: 0
beta: 0.5
# make sure that if you set want to use FISM to pretrain NAIS, your must set the pretrain path
# such as './saved/'
pretrain_path: ~