# 内部开发手册

## 如何开发一个新模型？

1. 确定要开发的模型
    1) 在下表中登记模型信息，包括模型名称，类别，模型状态，以及现负责人。
       
       模型名称：该模型常用的简称或全称
       
       模型类别：general （传统top-k推荐），context-aware （FM类等利用较多特征信息方法），
       sequential（序列推荐方法，包括next-item，next-session）。如有新的模型类别，请加入到此处。

       模型状态：按照开发程度由低到高分为：未认领（没有人认领），开发中（正在编写代码，模型没有办法在现有框架下跑通），
       调试中（编码基本完成，也可以顺利跑通，但未经过最后的测试），已上线（编写模型通过最后测试，上线成功）

       现负责人：保证每一个模型都有专门的负责人，负责开源之后的模型维护工作。当不处于这个项目后，需要把自己负责的模型进行转交，更新最新的负责人。

        |  序号  | 模型名称 | 模型类别 | 模型状态 | 现负责人 |
        |  ---- | ----  | ----  | ----  | ----  |
        | 1  | Popularity | general | 开发中 | 林子涵 |
        | 2  | ItemKNN | general | 开发中 | 林子涵 |
        | 3  | BPRMF | general | 测试中 | 牟善磊 |
        | 4  | NCF | general | 测试中 | 牟善磊 |
        | 5  | NGCF | general | 开发中 | 林子涵 |
        | 6  | LightGCN | general | 未认领 |  |
        | 7  | FM | context-aware | 开发中 | 牟善磊 |
        | 8  | DeepFM | context-aware | 开发中 | 牟善磊 |
        | 9  | NFM | context-aware | 开发中 | 林子涵 |
        | 10 | Wide&Deep | context-aware | 开发中 | 牟善磊 |
        | 11 | GRU4Rec | sequential | 开发中 | 牟善磊 |
        | 12 | SASRec | sequential | 未认领 |  |
        | 13 | FPMC | sequential | 未认领 |  |

2. 代码编写
    1) 在 model/模型类别/ 目录下新建py模型文件，若不存在相对应的模型类别目录，需要新建目录。
    2) 模型需要继承 AbstractRecommender 这个类，这个类要求实现 forward(), calculate_loss(), predict()三个方法。
    forward() 为模型前向传播逻辑，calculate_loss() 为训练时调用的方法，输入为PyTorch常见训练类型数据，输出为模型Loss。
    predict() 为评测时用到的方法，输出要为score或者其他任务适配的用来评测的内容。
    3) 编写模型文件前，可以查看一下是否有与待实现模型输入输出以及结构相似的模型，如果有可以参考进行编写，事半功倍。
    4) 在写模型结构时，包括各种layers和loss，如果为常见的layers和loss可以查看model/layers.py和model/loss.py看有没有已经实现的相应模块，
    如果有可以直接调用。如果没有且该模块非常常用，可以添加到layers.py或loss.py文件中。
    5) 模型相关的超参数，需要写入到配置文件中，配置文件目录：properties/model/
    6) 代码规范请参考 Python PEP8编码规范

3. 测试上线
    1) 首先需要保证模型能够顺利运行，可选取ml-100k这个数据集进行测试。简单测试方式:调整好数据集配置文件，模型配置文件，overall配置文件，
    在development_test/run_test1.py中设置要测试的模型，运行，检查是否报错。
    2) 保证模型顺利运行后，需要逐字检查模型文件，看是否有逻辑错误或其他错误。
    3) 在上线测试数据集上，进行训练评测。一种方式（推荐）：利用RecBox自动调参工具，按照模型类别找到相应的数据集及评测方式，设置好相应的配置文件，
    保证要测试的设置在run_test1.py中能够无误跑通，随后在development_test/hyper.test 中按照要求设置要调整的超参范围，调整好后执行run_test2.py，
    返回得到最优的超参和测试集结果，将结果填入下表中。另一种方式：自行调参。
    4) 检查3）得到的结果是否异常（是否与其他模型相差过大以及其他判断方式），正常无误模型可以顺利上线，结果异常需进一步检查代码，若还未发现问题，请
    及时与同学和老师进行沟通。
    
    注：
    1) 各类模型的上线评测标准（数据集，评测方式）还未制定。
    
        一个例子：模型类别：general，数据集：ml-1m，评测方式：8:1:1,全排序
    
        |  Method  | Recall@10 | NDCG@10 | Precision@10 |
        |  ---- | ----  | ----  | ----  |
        | ItemKNN  | 0.123 | 0.123 | 0.123 |
        | BPRMF  | 0.123 | 0.123 | 0.123 |
        | NCF  | 0.123 | 0.123 | 0.123 |